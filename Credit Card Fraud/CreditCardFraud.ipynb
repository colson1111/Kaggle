{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:  The goal of this project is to classify transactions into fraudulent or not fraudulent.\n",
    "Plan:\n",
    "1.  Examine the data - plot and look at distributions and correlation between variables\n",
    "2.  Transform the data - We expect that the principal components were normalized prior to being transformed by PCA.  We should consider transformation of the Amount column.\n",
    "3.  Split into test and train data\n",
    "4.  Modeling - We will try a number of different classification models.  Let's use 10-fold cross validation to tune any parameters if necessary.\n",
    "    *  Decision Tree (CART)\n",
    "    *  Logistic Regression\n",
    "    *  Support Vector Machine\n",
    "    *  Random Forest\n",
    "    *  XGBoost\n",
    "    *  Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function   # use print as a function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab                            # plot matplotlib plots inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dat = pd.read_csv(\"C:\\Users\\Craig\\Documents\\GitHubData\\creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...         V21       V22       V23       V24  \\\n",
      "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
      "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28  Amount  Class  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "(284807, 31)\n"
     ]
    }
   ],
   "source": [
    "# all variables are Principal Components and anonymized except Time (time since first transaction) and amount (amount of transaction)\n",
    "# When class == 1, it is a fraudulent transaction\n",
    "print(dat.head())\n",
    "print(dat.shape)  # 284,807 rows x 31 columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Look at where the class == 1\n",
    "dat.loc[dat['Class'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-56.407509631328999, 2.4549299912112099)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the range of V1?\n",
    "dat[\"V1\"].min(), dat[\"V1\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of V1\n",
    "# It appears that non-fraudulent purchases have more positive values and are not as left skewed as fradulent purchases\n",
    "positive = dat.loc[dat[\"Class\"] == 1]\n",
    "negative = dat.loc[dat[\"Class\"] != 1]\n",
    "n, bins, patches = plt.hist(positive[\"V1\"], 50, normed=1, facecolor='green', alpha = 0.75)\n",
    "n, bins, patches = plt.hist(negative[\"V1\"], 50, normed=1, facecolor='blue', alpha = 0.75)\n",
    "plt.xlabel(\"V1\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Histogram of V9\n",
    "# similar patter as V1\n",
    "positive = dat.loc[dat[\"Class\"] == 1]\n",
    "negative = dat.loc[dat[\"Class\"] != 1]\n",
    "n, bins, patches = plt.hist(positive[\"V9\"], 50, normed=1, facecolor='green', alpha = 0.75)\n",
    "n, bins, patches = plt.hist(negative[\"V9\"], 50, normed=1, facecolor='blue', alpha = 0.75)\n",
    "plt.xlabel(\"V1\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.6084975097731361e-17"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation between V1 and V2 (should be none bc they are principal components and therefore orthogonal)\n",
    "dat[\"V1\"].corr(dat[\"V2\"])  # -7.608e-17 - None!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into training and test sets, let's remove Time here bc I don't really want to mess with it now\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = dat[\"Class\"].values\n",
    "X = dat.drop([\"Class\", \"Time\"], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=802)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71202"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape  # 213,605 x 29\n",
    "X_test.shape   # 71,202 x 29\n",
    "len(y_train)   # 213,605\n",
    "len(y_test)    # 71,202"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will now standardize the Amount column - fit the standard scaler on the training data and then apply it to the test data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train) # fit to training data\n",
    "X_test_std = scaler.transform(X_test)       # apply to test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the unbalanced nature of the data, we will use AUC as a measure of accuracy rather than true positive rate or overall success rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ...,  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49908543448896892"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline AUC:  50% - if we assume NONE or ALL of the transactions are fraudulent, we'd be at 50% AUC.\n",
    "# Baseline AUC 2:  If we randomly choose 130 transactions as fraudulent, we'd be around 50% AUC (50.29%)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_true = y_test\n",
    "y_scores = np.zeros(len(y_true))\n",
    "y_scores = [y + 1 for y in y_scores]\n",
    "roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# We know there are 130 actual fraudulent transactions in the test set.  What we if we randomly choose 130 of the 71202?\n",
    "y_scores = np.zeros(len(y_true) - 130)\n",
    "one = np.ones(130)\n",
    "y_scores = np.append(y_scores,  one)\n",
    "\n",
    "# shuffle y_scores\n",
    "np.random.shuffle(y_scores)\n",
    "print(y_scores)\n",
    "\n",
    "roc_auc_score(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree (CART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88054410695113083"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decision tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Model\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model\n",
    "tree.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = tree.predict(X_test_std)\n",
    "\n",
    "# Decision Tree AUC: 87.29%\n",
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "* Baseline AUC:  50%\n",
    "* Decision Tree AUC:  87.28%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79607645984137432"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# initialize the model\n",
    "logistic = LogisticRegression(C=1000.0, random_state=802, penalty='l2', solver='lbfgs')\n",
    "\n",
    "# Train the model\n",
    "logistic.fit(X_train_std, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "test_pred = logistic.predict(X_test_std)\n",
    "\n",
    "# Logistic Regression AUC:  79.61%\n",
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "* Baseline AUC:          50.00%\n",
    "* Decision Tree (CART):  87.28%\n",
    "* Logistic Regression:   79.61%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# We have a ton of samples, let's use an out of the box SVC with linear kernel\n",
    "\n",
    "# initialize the model\n",
    "sv_class = SVC(kernel='linear')\n",
    "\n",
    "# fit the model on the training data\n",
    "sv_class.fit(X_train_std, y_train)\n",
    "\n",
    "# test prediction\n",
    "test_pred = sv_class.predict(X_test_std)\n",
    "\n",
    "# out of the box SVM with linear kernel, AUC:  89.22%\n",
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "* Baseline AUC:          50.00%\n",
    "* Decision Tree (CART):  87.28%\n",
    "* Logistic Regression:   79.61%\n",
    "* Linear SVM Classifier:  89.22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   21.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   21.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   21.9s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.4s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    7.2s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    4.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    8.1s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  9.0min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  8.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 18.9min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  6.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  6.8min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    6.8s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  6.7min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.3s finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 74.2min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed: 23.5min finished\n",
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89229362206906093"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# random forests are pretty good out of the box, but let's try tuning on the number of trees in the forest, \n",
    "# and the number of features randomly chosen at each tree split.\n",
    "\n",
    "# initialize the model\n",
    "rf = RandomForestClassifier(verbose=1)\n",
    "\n",
    "# set up grid search parameters\n",
    "param_grid = {\"n_estimators\": [100,500],\n",
    "              \"max_features\": [1, \"sqrt\", \"log2\"]}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(rf, param_grid=param_grid, verbose=1, n_jobs=3)\n",
    "\n",
    "# fit the model\n",
    "grid_search.fit(X_train_std, y_train)\n",
    "\n",
    "# predict\n",
    "test_pred = grid_search.predict(X_test_std)\n",
    "\n",
    "# Random Forest AUC:  89.23%  (500 trees, sqrt(n features) sampled\n",
    "roc_auc_score(y_test, test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n",
    "* Baseline AUC:          50.00%\n",
    "* Decision Tree (CART):  87.28%\n",
    "* Logistic Regression:   79.61%\n",
    "* Linear SVM Classifier: 89.22%\n",
    "* Random Forest:         89.23%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           0.0101            6.88m\n",
      "         2           0.0097            6.94m\n",
      "         3           0.0095            6.97m\n",
      "         4           0.0093            6.95m\n",
      "         5           0.0091            6.90m\n",
      "         6           0.0089            6.90m\n",
      "         7           0.0087            6.86m\n",
      "         8           0.0086            6.88m\n",
      "         9           0.0085            6.84m\n",
      "        10           0.0084            6.83m\n",
      "        20           0.0076            6.66m\n",
      "        30           0.0071            6.48m\n",
      "        40           0.0067            6.31m\n",
      "        50           0.0065            6.15m\n",
      "        60           0.0063            6.00m\n",
      "        70           0.0061            5.88m\n",
      "        80           0.0060            5.74m\n",
      "        90           0.0059            5.59m\n",
      "       100           0.0058            5.45m\n",
      "       200           0.0050            4.20m\n",
      "       300           0.0040            2.78m\n",
      "       400           0.0034            1.39m\n",
      "       500           0.0027            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89212477920548605"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# initialize the model - need to reduce the learning rate to avoid 'seesawing' farther and farther from the min\n",
    "gb = GradientBoostingClassifier(verbose=1, learning_rate=0.025, n_estimators=500)\n",
    "\n",
    "# fit the model\n",
    "gb.fit(X_train_std, y_train)\n",
    "\n",
    "# predict\n",
    "test_pred = gb.predict(X_test_std)\n",
    "\n",
    "# Random Forest AUC:  89.21%  (500 trees, 2.5% learning rate)\n",
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Classifier\n",
    "* Baseline AUC:          50.00%\n",
    "* Decision Tree (CART):  87.28%\n",
    "* Logistic Regression:   79.61%\n",
    "* Linear SVM Classifier: 89.22%\n",
    "* Random Forest:         89.23%\n",
    "* Gradient Boosting:     89.22%\n",
    "* Neural Network:        89.60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00604466\n",
      "Iteration 2, loss = 0.00295988\n",
      "Iteration 3, loss = 0.00262334\n",
      "Iteration 4, loss = 0.00243080\n",
      "Iteration 5, loss = 0.00222736\n",
      "Iteration 6, loss = 0.00212663\n",
      "Iteration 7, loss = 0.00197242\n",
      "Iteration 8, loss = 0.00181449\n",
      "Iteration 9, loss = 0.00175208\n",
      "Iteration 10, loss = 0.00161562\n",
      "Iteration 11, loss = 0.00158075\n",
      "Iteration 12, loss = 0.00149277\n",
      "Iteration 13, loss = 0.00153898\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.020000\n",
      "Iteration 14, loss = 0.00128761\n",
      "Iteration 15, loss = 0.00105539\n",
      "Iteration 16, loss = 0.00096347\n",
      "Iteration 17, loss = 0.00090381\n",
      "Iteration 18, loss = 0.00086748\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.004000\n",
      "Iteration 19, loss = 0.00078683\n",
      "Iteration 20, loss = 0.00076882\n",
      "Iteration 21, loss = 0.00075719\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000800\n",
      "Iteration 22, loss = 0.00073929\n",
      "Iteration 23, loss = 0.00073684\n",
      "Iteration 24, loss = 0.00073494\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000160\n",
      "Iteration 25, loss = 0.00073098\n",
      "Iteration 26, loss = 0.00073062\n",
      "Iteration 27, loss = 0.00073019\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000032\n",
      "Iteration 28, loss = 0.00072942\n",
      "Iteration 29, loss = 0.00072934\n",
      "Iteration 30, loss = 0.00072927\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000006\n",
      "Iteration 31, loss = 0.00072911\n",
      "Iteration 32, loss = 0.00072910\n",
      "Iteration 33, loss = 0.00072908\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000001\n",
      "Iteration 34, loss = 0.00072905\n",
      "Iteration 35, loss = 0.00072905\n",
      "Iteration 36, loss = 0.00072904\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Setting learning rate to 0.000000\n",
      "Iteration 37, loss = 0.00072904\n",
      "Iteration 38, loss = 0.00072904\n",
      "Iteration 39, loss = 0.00072904\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Learning rate too small. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.89609756519932116"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# initialize the model - try 4 hidden layers, reducing in size.\n",
    "# Use relu to speed things up, adaptive learning as loss decreases\n",
    "nn = MLPClassifier(hidden_layer_sizes=(1000,100,50), activation='relu',solver='sgd',\n",
    "                   learning_rate = 'adaptive', learning_rate_init = 0.1, max_iter=200,verbose=True)\n",
    "\n",
    "# fit the model\n",
    "nn.fit(X_train_std, y_train)\n",
    "\n",
    "# predict\n",
    "test_pred = nn.predict(X_test_std)\n",
    "\n",
    "# NN AUC:  89.61% (alpha=default, (100,100,50))\n",
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89220920063727349"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out of the box SVM AUC:\n",
    "roc_auc_score(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
